import warnings
from typing import Any, Dict

warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

import instructor  # noqa: E402
import litellm  # noqa: E402

from .prompts import (  # noqa: E402
    ImprovedResponse,
    RatingResponse,
    critique_prompt,
    rating_prompt,
    refine_prompt,
)
from .utils import normalize_rating_score  # noqa: E402


def generate_initial_answer(prompt: str, request_settings: Dict[str, Any]) -> str:
    """
    Generates the initial answer using greedy decoding.

    Args:
        prompt (str): The input prompt to guide the model's response.
        request_settings (Dict[str, Any]): A dictionary containing configuration settings
            for the model, including temperature, top_p, and other parameters.

    Returns:
        str: The initial response from the model as a string.
    """
    # greedy decoding parameters
    greedy_request_settings = request_settings.copy()
    greedy_request_settings["temperature"] = 0.0
    greedy_request_settings["top_p"] = 1.0

    return get_model_response(prompt, greedy_request_settings)


def generate_rating(prompt: str, answer: str, request_settings: Dict[str, Any]) -> float:
    """
    Generates a normalized rating score for a given answer based on a prompt and schema.

    Args:
        prompt (str): The original input prompt to guide the model's response.
        answer (str): The generated or improved answer to be rated.
        request_settings (Dict[str, Any]): A dictionary containing configuration settings
            for the model, such as API key, model name, and decoding parameters.

    Returns:
        float: A normalized rating score within the range [0, 0.95].
    """
    rating_response = get_structured_model_response(
        rating_prompt.format(original_prompt=prompt, improved_answer=answer),
        request_settings=request_settings,
        json_schema=RatingResponse,
    )
    rating = rating_response.rating

    # check type of the rating
    if not isinstance(rating, (int, str)):
        rating = str(rating)

    # normalize the rating to be within the range [0, 0.95]
    return normalize_rating_score(rating)


def generate_feedback(prompt: str, answer: str, request_settings: Dict[str, Any]) -> str:
    """
    Generates feedback for a given answer based on the provided prompt.

    Args:
        prompt (str): The original input prompt that guided the initial response.
        answer (str): The answer to be critiqued or evaluated.
        request_settings (Dict[str, Any]): A dictionary containing configuration settings
            for the model, such as API endpoint, model name, and decoding parameters.

    Returns:
        str: The generated feedback from the model as a string.
    """
    response = get_model_response(
        critique_prompt.format(original_prompt=prompt, initial_answer=answer), request_settings
    )

    if not isinstance(response, str):
        response = str(response)

    return response


def generate_improved_version(
    prompt: str, answer: str, feedback: str, request_settings: Dict[str, Any]
) -> str:
    """
    Generates an improved version of an answer based on the provided feedback.

    Args:
        prompt (str): The original input prompt that guided the generation of the answer.
        answer (str): The initial answer to be improved.
        feedback (str): Feedback provided for refining the answer.
        request_settings (Dict[str, Any]): A dictionary containing configuration settings
            for the model, such as API endpoint, model name, and decoding parameters.

    Returns:
        str: The improved version of the answer generated by the model.
    """
    improved_response = get_structured_model_response(
        refine_prompt.format(original_prompt=prompt, previous_answer=answer, feedback=feedback),
        request_settings=request_settings,
        json_schema=ImprovedResponse,
    )
    response = improved_response.ImprovedText

    if not isinstance(response, str):
        return str(response)

    return response


def get_model_response(prompt: str, request_settings: Dict[str, Any]) -> str:
    """
    Sends a prompt to the model and retrieves a textual response.

    Args:
        prompt (str): The input prompt or query to be sent to the model.
        request_settings (Dict[str, Any]): Configuration dictionary .

    Returns:
        str: The model's response as a string extracted from the first choice.
    """
    response = litellm.completion(
        messages=[{"content": prompt, "role": "user"}], **request_settings
    )

    return str(response["choices"][0]["message"]["content"])


def get_structured_model_response(
    prompt: str, request_settings: Dict[str, Any], json_schema: Any
) -> Any:
    """
    Sends a prompt to the model and retrieves a structured response based on a provided JSON schema.

    Args:
        prompt (str): The input prompt or query to be sent to the model.
        request_settings (Dict[str, Any]): Configuration dictionary.
        json_schema (Any): A Pydantic model used to validate the structured response.

    Returns:
        Any: The structured response validated against the provided JSON schema.
             Typically, this will be an instance of the schema model.
    """
    client = instructor.from_litellm(litellm.completion)

    request_settings["response_model"] = json_schema

    response = client.create(messages=[{"content": prompt, "role": "user"}], **request_settings)

    return response
